{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69619a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hyacinth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\Hyacinth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.EL2C6PLE4ZYW3ECEVIV3OXXGRN2NRFM2.gfortran-win_amd64.dll\n",
      "C:\\Users\\Hyacinth\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hyacinth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hyacinth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Hyacinth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from transformers import AutoModel, BertTokenizerFast,DistilBertTokenizerFast\n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "from datasets import load_dataset\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "words = set(nltk.corpus.words.words())\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import pytorch_lightning as pl\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "RANDOM_SEED = 0\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "pd.set_option('display.max_columns', None)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32c728b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, model_name=\"bert-base-cased\", batch_size=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        df = pd.read_csv('data/sentiment_app_reviews.csv')[:1000]\n",
    "        #train_data, val_data=train_test_split(dataset,test_size=0.2,random_state=RANDOM_SEED, stratify=dataset['Sentiment'])\n",
    "        \n",
    "        train_content, val_content,train_sentiments,val_sentiments= train_test_split(df['content'],df['sentiment_score'],test_size=0.2,random_state=RANDOM_SEED, stratify=df['sentiment'])\n",
    "        \n",
    "        self.train_content = train_content\n",
    "        self.train_sentiments = train_sentiments\n",
    "        self.val_content = val_content\n",
    "        self.val_sentiments = val_sentiments\n",
    "        \n",
    "    def tokenize_and_encode(self):\n",
    "        \n",
    "    # tokenize and encode sequences in the training set\n",
    "        tokens = tokenizer.batch_encode_plus(\n",
    "            content.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length = max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False\n",
    "        )\n",
    "        return tokens  \n",
    "    \n",
    "    def setup_tokens(self):\n",
    "        self.tokens_train=tokenize_and_encode(train_content)\n",
    "        self.tokens_val=tokenize_and_encode(val_content)\n",
    "\n",
    "\n",
    "    def tokenize_data(self, sample):\n",
    "        # processing the data\n",
    "        return self.tokenizer(\n",
    "            sample,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=128,\n",
    "        )\n",
    "    \n",
    "    def convert_to_tensor(self,tokens, sentiments):\n",
    "        sequence = torch.tensor(tokens['input_ids'])\n",
    "        mask = torch.tensor(tokens['attention_mask'])\n",
    "        y = torch.tensor(sentiments.tolist())\n",
    "        return sequence, mask, y\n",
    "    \n",
    "    def setup_tensor_convertor(self):\n",
    "        self.train_sequence,self.train_mask,self.train_y=convert_to_tensor(self.tokens_train,self.train_sentiments)\n",
    "        self.val_sequence,self.val_mask,self.val_y=convert_to_tensor(self.tokens_val,self.val_sentiments)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        train_data = TensorDataset(self.train_sequence, self.train_mask, self.train_y)\n",
    "        train_sampler = RandomSampler(self.train_data)\n",
    "        train_dataloader = DataLoader(self.train_data, sampler=self.train_sampler, batch_size=self.batch_size)\n",
    "        return train_dataloader\n",
    "        \n",
    "        \n",
    "    def val_dataloader(self):\n",
    "        val_data = TensorDataset(self.val_sequence, self.val_mask, self.val_y)\n",
    "        val_sampler = SequentialSampler(self.val_data)\n",
    "        val_dataloader = DataLoader(self.val_data, sampler = self.val_sampler, batch_size=self.batch_size)\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b8cc602",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'content' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m data \u001b[38;5;241m=\u001b[39m DataModule()\n\u001b[0;32m      2\u001b[0m data\u001b[38;5;241m.\u001b[39mprepare_data()\n\u001b[1;32m----> 3\u001b[0m data\u001b[38;5;241m.\u001b[39mtokenize_and_encode(\u001b[43mcontent\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'content' is not defined"
     ]
    }
   ],
   "source": [
    "data = DataModule()\n",
    "data.prepare_data()\n",
    "data.tokenize_and_encode(content)\n",
    "data.setup_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab6ebcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8e185e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_and_encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36mDataModule.setup_tokens\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup_tokens\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_train\u001b[38;5;241m=\u001b[39m\u001b[43mtokenize_and_encode\u001b[49m(train_content)\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokens_val\u001b[38;5;241m=\u001b[39mtokenize_and_encode(val_content)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize_and_encode' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c065e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7781343",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c0e70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eca342b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
